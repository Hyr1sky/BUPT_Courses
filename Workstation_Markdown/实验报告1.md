## Assignment 1

### *TASK*:

> (3）设X为区间 $[0,100)$ 中的P个均匀采样点,$Y=(X+5)^2+10+N$，N是均值为 $\mu$、标
  准差为 $\sigma$ 的高斯随机噪声。试选取不同的P、$\mu$、$\sigma$ 值，构造X和Y数据集，然后分别通过线性回归学习得到X和Y之间的不同的回归方程，并分析不同取值对回归方程误差的影响。
> (4）试分别采用逻辑回归、贝叶斯分类器、SVM分类器和决策树分类器等算法，针对手写
  数字数据集进行训练,并验证不同算法的性能。
> (5）试分别采用K均值、亲和传播、谱聚类和 DBSCAN 等算法，针对手写数字数据集进行聚类，并验证不同算法的聚类性能。

### Question 3
```python
import numpy as np
from sklearn. linear_model import LinearRegression
from sklearn. model_selection import train_test_split
import matplotlib. pyplot as plt

#设置参数
p = 1000 #采样点数
a = 0 #均值
b = 1 #方差
#在区间(0，100)中生成p个均匀采样点
x = np.linspace(0, 100, p)
#生成y
n = np.random.normal(a, b, p)
y = (x + 5)**2 + 10 + n
#将数据整形为线性回归所需格式
x = x.reshape(-1, 1)
y = y.reshape(-1, 1)
#将数据集分为训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)
#使用线性回归拟合数据
model = LinearRegression()
model.fit(x_train,y_train)
#预测
y_pred = model.predict(x_test)
#输出线性方程
print(f"线性方程: y = {model.coef_[0][0]}*x + {model.intercept_[0]}")
#可视化结果
plt.scatter(x_test, y_test, color = 'blue', label = '实际数据')
plt.plot(x_test, y_pred, color = 'red', label = '预测结果')
plt.xlabel('x')
plt.ylabel('y')
plt.legend()
plt.show()
```

#### Output:

![[Assignment1 Q3.png]]

---


### Question 4

1. [LR,SVM,决策树的对比](https://blog.csdn.net/qq_38147421/article/details/120195049)
2. [朴素贝叶斯分类器](https://blog.csdn.net/sinat_36246371/article/details/60140664)

## 图片文件格式说明：

---
| 字节位置 |    类型    |  值   |  描述  |
|:--------:|:----------:|:-----:|:------:|
|   0000   |  32位整型  | 2051  |  幻数  |
|   0004   |  32位整型  | 60000 | 图片数 |
|   0008   |  32位整型  |  28   |  行数  |
|   0012   |  32位整型  |  28   |  列数  |
|   0016   | 无符号字节 |  ??   |  像素  |
|   0017   | 无符号字节 |  ??   |  像素  |
|  ......  |            |       |        |
|   xxxx   | 无符号字节 |  ??   |  像素  |

----------------------------------------
| 字节位置 |    类型    |  值   |  描述  |
|:--------:|:----------:|:-----:|:------:|
|   0000   |  32位整型  | 2051  |  幻数  |
|   0004   |  32位整型  | 60000 | 图片数 |
|   0008   |  32位整型  |  28   |  行数  |
|   0012   |  32位整型  |  28   |  列数  |
|   0016   | 无符号字节 |  ??   |  像素  |
|   0017   | 无符号字节 |  ??   |  像素  |
|  ......  |            |       |        |
|   xxxx   | 无符号字节 |  ??   |  像素  |

----------------------------------------

注：这里的整形指的都是无符号整型

上述的32位整形遵循MSB first，即高位字节在左边，如十进制8，二进制储存形式为1000。

幻数是一个固定值，它占据文件的前4个字节，实际上表示的是这个文件储存的是图片还是标签，没有具体用处，我们可以忽略它。

图片数与标签数占据文件4~7个字节的位置，在训练集中，它为60,000，表示这个文件有60,000个图片或标签，在测试集中，它为5,000。

行数和列数描述的是每张图片的大小，它们也是固定值，都为28。

每张图片有$28*28=784$个像素，所以从图片文件第16个字节位置开始，每隔784个字节为一张新图片，其中每个像素的像素值为0~255。

从标签文件的第8个字节位置开始，每个字节都对应着一张图片的数字，标签的值为0~9。

---

### 1. Logistic Regression:

**Code:**
```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

if __name__ == '__main__':
    mnist = load_digits()
    x,test_x,y,test_y = train_test_split(mnist.data, mnist.target, test_size = 0.25, random_state = 40)
    
    model = LogisticRegression()
    model.fit(x, y)
    acc = model.score(test_x, test_y)
    print("The accuracy is %.3f" % acc)
    z = model.predict(test_x)

    print('准确率:',np.sum(z == test_y)/z.size)
```
**Output:**
```
The accuracy is 0.964
准确率：0.964444444444
```

### 2. Naïve Bayes Classifier:

**Code:**
```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import train_test_split

if __name__ == '__main__':
    mnist = load_digits()
    x,test_x,y,test_y = train_test_split(mnist.data, mnist.target, test_size = 0.25, random_state = 40)
    
    model1 = GaussianNB()
    model1.fit(x, y)
    acc1 = model1.score(test_x, test_y)
    print("GuassianNB accuracy is %.3f" % acc1)
    z1 = model1.predict(test_x)
    print('准确率:',np.sum(z1 == test_y)/z1.size)

    model2 = MultinomialNB()
    model2.fit(x, y)
    acc2 = model2.score(test_x, test_y)
    print("MultionmialNB accuracy is %.3f" % acc2)
    z2 = model2.predict(test_x)
    print('准确率:',np.sum(z2 == test_y)/z2.size)

    model3 = BernoulliNB()
    model3.fit(x, y)
    acc3 = model3.score(test_x, test_y)
    print("BernoulliNB accuracy is %.3f" % acc3)
    z3 = model3.predict(test_x)
    print('准确率:',np.sum(z3 == test_y)/z3.size)
```
**Output:**
```
GuassianNB accuracy is 0.847 
准确率: 0.8466666666666667 
MultionmialNB accuracy is 0.860 
准确率: 0.86 
BernoulliNB accuracy is 0.829 
准确率: 0.8288888888888889
```

### 3. SVM Classifier:

**Code:**
```python
import numpy as np
from sklearn import svm
from sklearn.datasets  import load_digits
from sklearn.model_selection  import train_test_split
# import _pickle as pickle

if __name__ == '__main__':
    mnist = load_digits()
    x,test_x,y,test_y = train_test_split(mnist.data, mnist.target, test_size = 0.25, random_state = 40)
    
    model = svm.LinearSVC()
    model.fit(x, y)

    z = model.predict(test_x)

    print('准确率:',np.sum(z == test_y)/z.size)

#   with open('./model.pkl','wb') as file:
#       pickle.dump(model,file)
```
**Output:**
```
准确率: 0.9333333333333333
```

### 4. Decision Tree:

**Code:**
```python
import numpy as np
from sklearn.datasets import load_digits
from sklearn import tree
from sklearn.model_selection import train_test_split

if __name__ == '__main__':
    mnist = load_digits()
    x,test_x,y,test_y = train_test_split(mnist.data, mnist.target, test_size = 0.25, random_state = 40)
    
    model = tree.DecisionTreeClassifier()
    model.fit(x, y)
    acc = model.score(test_x, test_y)
    print("The accuracy is %.3f" % acc)
    z = model.predict(test_x)

    print('准确率:',np.sum(z == test_y)/z.size)
```
**Output:**
```
The accuracy is 0.820 准确率: 0.82
```

### 代码(by GPT4)

```python
import torch
import torchvision
from torchvision import transforms

# 定义数据转换
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])

# 加载训练集
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)

# 加载测试集
test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score

# 转换数据
X_train = train_dataset.data.numpy()
y_train = train_dataset.targets.numpy()
X_test = test_dataset.data.numpy()
y_test = test_dataset.targets.numpy()

# 重塑数据以适应 Scikit-learn 模型
X_train = X_train.reshape(-1, 28*28)
X_test = X_test.reshape(-1, 28*28)

# 训练模型
models = [
    ('Logistic Regression', LogisticRegression(max_iter=10000)),
    ('Naive Bayes', GaussianNB()),
    ('SVM', SVC(kernel='linear')),
    ('Decision Tree', DecisionTreeClassifier())
]

for name, model in models:
    model.fit(X_train, y_train)
    
    # 验证模型性能
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"{name} Accuracy: {accuracy:.2%}")
```

这段代码首先定义了数据转换，将图像转换为 PyTorch 张量并进行标准化。接着，它使用 torchvision.datasets.MNIST 类加载了训练集和测试集，并将它们分别存储在 train_loader 和 test_loader 中。这些数据加载器可用于在训练和评估过程中迭代数据。

现在，你可以使用 PyTorch 构建和训练神经网络模型。如果你希望将这些数据集与 Scikit-learn 的模型结合使用，请注意需要将 PyTorch 张量转换为 Numpy 数组，然后将数据重塑为适合 Scikit-learn 模型输入的格式。

你可以使用 PyTorch 加载的数据集（MNIST）进行训练和验证。首先，我们需要将 PyTorch 张量转换为 Numpy 数组，并将图像数据重塑为适合 Scikit-learn 模型输入的格式。然后，我们可以像之前一样使用 Scikit-learn 库训练逻辑回归、贝叶斯分类器、SVM 分类器和决策树分类器，并评估它们的性能。

这段代码将输出每个模型在 MNIST 数据集上的准确性。请注意，训练和测试大型数据集可能需要一些时间，特别是在计算资源有限的情况下。你可以根据实际需求调整模型参数以获得更好的性能。